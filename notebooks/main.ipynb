{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Live project\n",
    "\n",
    "## 1. Extracting City and Country Information from News Headlines\n",
    "\n",
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Builtin Python modules\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import re\n",
    "import string\n",
    "from typing import Dict, List, Set\n",
    "import unicodedata\n",
    "\n",
    "from geonamescache import GeonamesCache\n",
    "from more_itertools import partitions\n",
    "import pandas as pd\n",
    "\n",
    "from helper import Cities, Countries, City, Country"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load data\n",
    "\n",
    "Get information from helper classes and load data from `./data/headlines.txt` file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Cities\n",
    "helper_cities = Cities(\"../data/cities.json\", GeonamesCache())\n",
    "group_cities_by_word_count: Dict[int, List[str]] = helper_cities.load_json()\n",
    "cities: Dict[str, City] = helper_cities.get_cities()\n",
    "\n",
    "# Countries\n",
    "helper_countries = Countries(\"../data/countries.json\", GeonamesCache())\n",
    "group_countries_by_word_count: Dict[int, List[str]] = helper_countries.load_json()\n",
    "countries: Dict[str, Country] = helper_countries.get_countries()\n",
    "\n",
    "text_file: Path = Path('../data/headlines.txt')\n",
    "assert text_file.is_file(), f\"Wrong file: {text_file}\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize several variables\n",
    "\n",
    "* Get unique cities.\n",
    "* Create a dictionary which keys are country codes and values a list of cities within that country.\n",
    "* Create a translation table to remove certain characters."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "unique_cities: Set[str] = set()\n",
    "map_country_code_to_city: Dict[str, List[str]] = {}\n",
    "\n",
    "extra_characters = [\"-\", \"â€™\", \"/\"]\n",
    "valid_characters: Set[str] = set(\n",
    "    [str(character) for character in [*string.ascii_letters, *string.digits, *extra_characters]])\n",
    "\n",
    "for city in cities.values():\n",
    "    # Country code as dictionary search_term_length\n",
    "    country_code = city['countrycode']\n",
    "    if country_code not in map_country_code_to_city.keys():\n",
    "        map_country_code_to_city[country_code] = []\n",
    "    # Append city name\n",
    "    city_name = city['name']\n",
    "    if city_name not in map_country_code_to_city[country_code]:\n",
    "        map_country_code_to_city[country_code].append(city_name)\n",
    "        unique_cities.update([city_name])\n",
    "        for character in city_name:\n",
    "            valid_characters.update([character])\n",
    "        country_name = countries[country_code]['name']\n",
    "        for character in country_name:\n",
    "            valid_characters.update([character])\n",
    "\n",
    "lines: List[str] = []  # 650 lines\n",
    "\n",
    "all_characters: Set[str] = set()\n",
    "\n",
    "with text_file.open(encoding=\"utf8\") as file_handler:\n",
    "    for line in file_handler.readlines():\n",
    "        modified_line = line.rstrip('\\n')\n",
    "        lines.append(modified_line)\n",
    "        for character in modified_line:\n",
    "            all_characters.update([character])\n",
    "\n",
    "nonvalid_characters: Set[str] = all_characters - valid_characters\n",
    "# Create translation table\n",
    "nonvalid_translation: str = \"\".join(list(nonvalid_characters))\n",
    "translation_table = dict.fromkeys(map(ord, nonvalid_translation), None)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Helper methods\n",
    "\n",
    "Replace foreign characters with normalized ascii characters and partition text lines to create several search terms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def normalize_text(text: str) -> str:\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def get_search_terms(current_line: str) -> Dict[int, Set[str]]:\n",
    "    # Remove unwanted characters\n",
    "    modified_line = current_line.translate(translation_table)\n",
    "    # Remove multiple spaces\n",
    "    modified_line = re.sub(r\"\\s\\s+\", \" \", modified_line)\n",
    "    words = [word.rstrip(\",\") for word in modified_line.split(\" \")]\n",
    "\n",
    "    results: Dict[int, Set[str]] = {}\n",
    "    for partition in partitions(words):\n",
    "        for subpartition in partition:\n",
    "            subpartition_word_count: int = len(subpartition)\n",
    "            if subpartition_word_count not in results.keys():\n",
    "                results[subpartition_word_count] = set()\n",
    "            result = \" \".join(subpartition)\n",
    "            results[subpartition_word_count].update([result])\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Iterate over text lines\n",
    "\n",
    "For each line try to find a city and a country and append it within a multi level list."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Data-frame data\n",
    "df_data: List[List[str]] = []\n",
    "\n",
    "for line in lines:\n",
    "    search_terms: Dict[int, Set[str]] = get_search_terms(line)\n",
    "\n",
    "    # Create a dictionary with cities as keys and country codes as values\n",
    "    found_cities: Dict[str, str] = {}\n",
    "    found_countries: Set[str] = set()\n",
    "\n",
    "    for search_term_length, search_term in search_terms.items():\n",
    "        if search_term_length in sorted(group_cities_by_word_count.keys()):\n",
    "            # Create a dictionary with normalized cities as keys and country codes as values\n",
    "            cities_and_countries: Dict[str, str] = {normalize_text(current[:-3]): current[-2:] for current in\n",
    "                                                    group_cities_by_word_count[search_term_length]}\n",
    "            # Create a set of normalized cities from previous dictionary\n",
    "            subgroup_cities_by_word_count: Set[str] = set(\n",
    "                [normalize_text(city_key) for city_key in cities_and_countries.keys()])\n",
    "            # Compare original search term with normalized cities\n",
    "            intersections: Set[str] = search_term.intersection(subgroup_cities_by_word_count)\n",
    "            if len(intersections) > 0:\n",
    "                longest_word = max(list(intersections), key=len)\n",
    "                country_iso = cities_and_countries[longest_word]\n",
    "                # Compare original search term with non normalized cities\n",
    "                original_city = [current[:-3] for current in group_cities_by_word_count[search_term_length] if\n",
    "                                 normalize_text(current[:-3]) == longest_word][0]\n",
    "                found_cities[original_city] = countries[country_iso]['name']\n",
    "        if search_term_length in group_countries_by_word_count.keys():\n",
    "            subgroup_countries_by_word_count: Set[str] = set(group_countries_by_word_count[search_term_length])\n",
    "            intersections: Set[str] = search_term.intersection(subgroup_countries_by_word_count)\n",
    "            if len(intersections) > 0:\n",
    "                found_countries.update(intersections)\n",
    "\n",
    "    # Remove duplicate countries\n",
    "    longest_word = \"\"\n",
    "    if len(found_cities) > 0:\n",
    "        longest_word = max(found_cities.keys(), key=len)\n",
    "    if longest_word in found_cities:\n",
    "        found_countries.clear()\n",
    "        # Add country\n",
    "        found_countries.add(found_cities[longest_word])\n",
    "\n",
    "    df_data.append([line, \", \".join(list(found_countries)), longest_word])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Panda's data-frame\n",
    "\n",
    "Create a new data-frame and export it to a CSV file (`../data/clean.csv`)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "                                  headline      countries         cities\n0                 Zika Outbreak Hits Miami  United States          Miami\n1          Could Zika Reach New York City?  United States  New York City\n2        First Case of Zika in Miami Beach  United States    Miami Beach\n3  Mystery Virus Spreads in Recife, Brazil         Brazil         Recife\n4  Dallas man comes down with case of Zika  United States         Dallas",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>headline</th>\n      <th>countries</th>\n      <th>cities</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Zika Outbreak Hits Miami</td>\n      <td>United States</td>\n      <td>Miami</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Could Zika Reach New York City?</td>\n      <td>United States</td>\n      <td>New York City</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>First Case of Zika in Miami Beach</td>\n      <td>United States</td>\n      <td>Miami Beach</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Mystery Virus Spreads in Recife, Brazil</td>\n      <td>Brazil</td>\n      <td>Recife</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Dallas man comes down with case of Zika</td>\n      <td>United States</td>\n      <td>Dallas</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_names = ['headline', 'countries', 'cities']\n",
    "live_project_df = pd.DataFrame(df_data, columns=column_names)\n",
    "\n",
    "# Export to CSV\n",
    "live_project_df.to_csv(\"../data/clean.csv\", index=False, quoting=csv.QUOTE_ALL)\n",
    "live_project_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}